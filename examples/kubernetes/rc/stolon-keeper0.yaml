apiVersion: v1
kind: ReplicationController
metadata:
  name: stolon-keeper0
spec:
  ## the number of replicas must be 1
  replicas: 1
  ## don't do rolling upgrades on update
  selector:
    name: stolon-keeper
  template:
    metadata:
      labels:
        name: stolon-keeper
        stolon-cluster: "kube-stolon"
        stolon-keeper: "true"
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: stolon-keeper
        image: sorintlab/stolon:v0.7.0-pg9.6
        command:
          - "/bin/bash"
          - "-ec"
          - |
            export POD_IP=$(hostname -i)
            export STKEEPER_PG_LISTEN_ADDRESS=$POD_IP
            export STOLON_DATA=/stolon-data
            chown stolon:stolon $STOLON_DATA
            exec gosu stolon stolon-keeper --data-dir $STOLON_DATA
        env:
          - name: STKEEPER_UID
            value: "keeper0"
          - name: STKEEPER_CLUSTER_NAME
          # TODO(sgotti) Get cluster name from "stoloncluster" label using a downward volume api instead of duplicating the name here?
            value: "kube-stolon"
          - name: STKEEPER_STORE_BACKEND
            value: "etcd" # Or consul
          - name: STKEEPER_STORE_ENDPOINTS
            value: "http://192.168.122.1:2379"
          - name: STKEEPER_PG_REPL_USERNAME
            value: "repluser"
            # Or use a password file like in the below supersuser password
          - name: STKEEPER_PG_REPL_PASSWORD
            value: "replpassword"
          - name: STKEEPER_PG_SU_USERNAME
            value: "stolon"
          - name: STKEEPER_PG_SU_PASSWORDFILE
            value: "/etc/secrets/stolon/password"
          ## Uncomment this to enable debug logs
          #- name: STKEEPER_DEBUG
          #  value: "true"
        ports:
          - containerPort: 5432
        volumeMounts:
        - mountPath: /stolon-data
          name: data
        - mountPath: /etc/secrets/stolon
          name: stolon
      volumes:
        - name: stolon
          secret:
            secretName: stolon
      ## Define your own persistent volume type. For testing we use an hostPath, but this shouldn't be used in production and won't work in multi-node cluster. See below example for using a persistentVolumeClaim.
        - name: data
          hostPath:
              path: /data/keeper0-data
    ## In production you should use a persistentVolumeClaim and configure your persistent volumes (statically or dynamic using a provisioner, see k8s doc)
    ##    - name: data
    ##      persistentVolumeClaim:
    ##        claimName: keeper0-data-pv-claim
